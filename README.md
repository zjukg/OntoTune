
# OntoTune
<!-- - [OntoTune: Ontology-Driven Self-training for Aligning Large Language Models](https://arxiv.org/abs/2) -->
>In this work, we propose an ontology-driven self-training framework called OntoTune, which aims to align LLMs with ontology through in-context learning, enabling the generation of responses guided by the ontology.

<div align="center">
    <img src="https://github.com/zjukg/OntoTune/blob/main/figure/overview.png" width="90%" height="auto" />
</div>

## üîî News
- **`2025-01` OntoTune is accepted by WWW 2025 !**


## üöÄ How to start
```bash
git clone https://github.com/zjukg/OntoTune.git
```
The code of fine-tuning is constructed based on open-sourced repo [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory).

### Dependencies
```bash
cd LLaMA-Factory
pip install -e ".[torch,metrics]"
```
### Data Preparation
1. The supervised instruction-tuned data generated by LLaMA3 8B for the LLM itself is placed in the [link]().
2. Put the downloaded `OntoTune_sft.json` file under `LLaMA-Factory/data/` directory.
3. Evaluation datasets for hypernym discovery and medical question answering are in `LLaMA-Factory/data/evaluation_HD` and `LLaMA-Factory/data/evaluation_QA`, respectively.

### Finetune LLaMA3
You need to add `model_name_or_path` parameter to yaml file„ÄÇ
```bash
cd LLaMA-Factory
llamafactory-cli train OntoTune_sft.yaml
```

## ü§ù Cite:
Please consider citing this paper if you find our work useful.

```bigquery

@inproceedings{OntoTune,
  author       = {Zhiqiang Liu and
                  Chengtao Gan and 
                  Junjie Wang and 
                  Yichi Zhang and
                  Zhongpu Bo and
                  Mengshu Sun and
                  Huajun Chen and
                  Wen Zhang},
  title        = {OntoTune: Ontology-Driven Self-training for Aligning Large Language Models},
  booktitle    = {{WWW}},
  year         = {2025}
}

```